{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# data\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#connect to database\n",
    "mongo_client = MongoClient()\n",
    "text_db = mongo_client.s2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['videos']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "videos = text_db.videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2166"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error\n",
    "It's clear that something went wrong during the transfer of information from the speech API into text and passing those documents into MongoDB. There were 53 videos, so there should be 53 documents. This is something I need to adddress and reprocess/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#extract all of the text so we can put it into Gensim\n",
    "text_list = []\n",
    "for doc in videos.find():\n",
    "    text_list.append(doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "\n",
    "#add some extra stop words that we don't want to become features\n",
    "basic_words = ['number', 'numbers', 'just', 'solution', 'answer',\n",
    "               'negative', 'know', 'don', 'going', 'want', 'like',\n",
    "               'work', 'question', 'say', 'think', 'maybe',\n",
    "              'problem', 'problems', 'equal', 'right', 'actually',\n",
    "              'really', 'let', 'need', 've', 'real', 'way', 'lot',\n",
    "              'good', 'equals', 'time', 'things', 'come', 'make', 'll'\n",
    "              'thing', 'pretty']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(basic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None,\n",
       "        stop_words=frozenset({'ours', 'seem', 'about', 'actually', 'been', 'several', 'problems', 'found', 'towards', 'still', 'pretty', 'along', 'equals', 'front', 'everywhere', 'things', 'alone', 'this', 'neither', 'solution', 'hereby', 'noone', 'as', 'whether', 'other', 'couldnt', 'thereby', 'might', 'hi...'besides', 'lot', 'everything', 'during', 'interest', 'three', 'anything', 'our', 'against', 'see'}),\n",
       "        strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating word count vector\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3),  \n",
    "                                   stop_words=stop_words, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "count_vectorizer.fit(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counts = count_vectorizer.transform(text_list).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48544, 2166)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sparse matrix to corpus object\n",
    "corpus = matutils.Sparse2Corpus(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# store row # and the word/n-gram it contains\n",
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, num_topics=3, id2word=id2word, passes=20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"people\" + 0.002*\"data\" + 0.002*\"interview\" + 0.002*\"talk\" + 0.001*\"solve\" + 0.001*\"interviewing\" + 0.001*\"natural\" + 0.001*\"did\" + 0.001*\"interviews\" + 0.001*\"job\"'),\n",
       " (1,\n",
       "  '0.004*\"people\" + 0.003*\"data\" + 0.002*\"questions\" + 0.002*\"interview\" + 0.001*\"ask\" + 0.001*\"point\" + 0.001*\"kind\" + 0.001*\"ll\" + 0.001*\"understand\" + 0.001*\"easy\"'),\n",
       " (2,\n",
       "  '0.003*\"list\" + 0.002*\"start\" + 0.002*\"value\" + 0.002*\"end\" + 0.002*\"case\" + 0.002*\"data\" + 0.002*\"interview\" + 0.002*\"doing\" + 0.002*\"return\" + 0.002*\"candidates\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Conclusion\n",
    "In retrospect, the topics generated by my choice of content are almost exclusively determined by the title/subject of the video. For example, if you were to explain linear regression in an interview question, you may only hear/say the phrase \"linear regression\" once or twice. It will be significantly outweighed by the smaller concepts that make up its building blocks. This is interesting in its own regard, but is part of the reaon this project was not able to accomplish what it initially set out to do. \n",
    "\n",
    "Further more, garbage in = garbage out. The number and quality of the videos used in this project was not compatible. Had I used many, many more the variety/scope of the questions/interviews would have been acceptable, but with a smaller sample size meaningful results could have only been generated with very targeted video selection. \n",
    "\n",
    "If I could alter this project, I would most likely use the Speech API for (real-time) sentiment analysis. Sentiment analysis is very straightforward and the \"quality\" of content is not under such stringent requirements. \n",
    "\n",
    "Also, there is a huge opportunity to make a YouTube channel that focuses on data science interview questions, coding problems, explaining different topics, etc etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
